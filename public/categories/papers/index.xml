<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>papers on Yuan Meng</title>
    <link>https://www.yuan-meng.com/categories/papers/</link>
    <description>Recent content in papers on Yuan Meng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Yuan Meng</copyright>
    <lastBuildDate>Fri, 21 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.yuan-meng.com/categories/papers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sift Through the Haystack: Vector Retrieval</title>
      <link>https://www.yuan-meng.com/posts/vector_retrieval/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.yuan-meng.com/posts/vector_retrieval/</guid>
      <description>Embeddings, Embeddings Everywhere Be it a person, a product, a place, a text, an image, or a planet &amp;mdash; virtually all entities you can think of can be represented as a special kind of vectors, called &amp;ldquo;embeddings.&amp;rdquo; Embedding is a classic idea in mathematical topology and machine learning (click â–¶ for definitions), recently made popular by the rise of foundation models that are exceptionally good at embedding texts, images, and videos to empower downstream use cases, such as embedding-based retrieval, text/image/video understanding, and deep learning rankers with embedding features, to name a few.</description>
    </item>
    
    <item>
      <title>Attention as Soft Dictionary Lookup</title>
      <link>https://www.yuan-meng.com/posts/attention_as_dict/</link>
      <pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.yuan-meng.com/posts/attention_as_dict/</guid>
      <description>The Dictionary Metaphor ðŸ”‘ðŸ“š By now, the scaled-dot product attention formula might have burned into our brains ðŸ§ , $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$, yet still this brilliant metaphor from Kevin Murphy&amp;rsquo;s PML book gives it a refreshed interpretation &amp;mdash;
 &amp;ldquo;We can think of attention as a soft dictionary look up, in which we compare the query $q$ to each key $k_i$, and then retrieve the corresponding value $v_i$.&amp;rdquo; &amp;mdash; Chapter 15, p.</description>
    </item>
    
    <item>
      <title>An Evolution of Learning to Rank</title>
      <link>https://www.yuan-meng.com/posts/ltr/</link>
      <pubDate>Sat, 17 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.yuan-meng.com/posts/ltr/</guid>
      <description>First Thing First  Enigmas of the universe  Cannot be known without a search  &amp;mdash; Epica, Omega (2021)
 In The Rainmaker (1997), the freshly graduated lawyer Rudy Baylor faced off against a giant insurance firm in his debut case, almost getting buried by mountains of case files that the corporate lawyers never expected him to sift through. If only Rudy had a search engine that retrieves all files mentioning suspicious denials and ranks them from most to least relevant, the case prep would&amp;rsquo;ve been a breeze.</description>
    </item>
    
    <item>
      <title>Autocompletion for Search Enginees</title>
      <link>https://www.yuan-meng.com/posts/autocomplete/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.yuan-meng.com/posts/autocomplete/</guid>
      <description>Autocompletion dates back half a century ago (Longuet-Higgins &amp;amp; Ortony, 1968), initially designed to save keystrokes as people type and help those with physical disabilities type faster. The incomplete user input is the &amp;ldquo;query prefix&amp;rdquo; and suggested ways of extending the prefix into a full query are &amp;ldquo;query completions&amp;rdquo;. This feature is essential to modern text editors and search engines.
 This blog post summarizes key ideas from the survey paper A Survey of Query Auto Completion in Information Retrieval, recommended by my Search teammate at DoorDash.</description>
    </item>
    
    <item>
      <title>The Ph.D. Grind: Doing Nothing Is the Hardest Thing</title>
      <link>https://www.yuan-meng.com/posts/nothingness/</link>
      <pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.yuan-meng.com/posts/nothingness/</guid>
      <description>Seriously, Read The Ph.D. Grind Everyone doing a Ph.D. or considering doing one should read The Ph.D. Grind. Don&amp;rsquo;t wait &amp;mdash; you&amp;rsquo;ll learn the much-needed &amp;ldquo;rational optimism&amp;rdquo; for this crazy ride.
I only came across this gem in my final semester year of Ph.D. but Dr. Philip Guo&amp;rsquo;s reflections of his first year in grad school immediately resonated with my own. I, too, procrastinated heavily for the first time in my life and felt I somehow lost my sense of direction.</description>
    </item>
    
    <item>
      <title>Latest Hits in Cognitively Inpsired AI</title>
      <link>https://www.yuan-meng.com/posts/cocosci/</link>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.yuan-meng.com/posts/cocosci/</guid>
      <description>This paper collection reflects some of the most exiting trends in computational cognitive science and cognitively inspired AI, in my opinion.
 Overview How can AI and cognitive science shed light on each other&amp;rsquo;s biggest challenges?
 Lake, B. M., Ullman, T. D., Tenenbaum, J. B., &amp;amp; Gershman, S. J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40. (PDF) Ullman, T. D., &amp;amp; Tenenbaum, J.</description>
    </item>
    
  </channel>
</rss>
